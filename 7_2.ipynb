{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "import graphviz\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "## import pyclustertend \n",
    "import matplotlib.cm as cm\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv', encoding = \"latin1\")\n",
    "file = open('cuantitativas.txt', 'r')\n",
    "quant= file.read().splitlines()\n",
    "file = open('cualitativas.txt', 'r')\n",
    "quali= file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de interes\n",
    "print('\\033[36m' + 'Kurtosis: %f' % data['SalePrice'].kurt())\n",
    "print('\\033[36m' + 'Asimetría: %f' % data['SalePrice'].skew())\n",
    "data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat,p = stats.shapiro(data[[\"SalePrice\"]].dropna())\n",
    "print('Prueba de Kolmogorov-Smirnov:\\np=%f\\n'% p)\n",
    "ks_statistic, p_value = diag.lilliefors(data[[\"SalePrice\"]].dropna())\n",
    "print('Prueba de Lilliefors:\\nks=%f\\np=%f'%(ks_statistic,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data['SalePrice'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)\n",
    "minPrice = data['SalePrice'].min()\n",
    "maxPrice = data['SalePrice'].max()\n",
    "divs = (maxPrice - minPrice) / 3\n",
    "data['priceRange'] = data['LotArea']\n",
    "\n",
    "data['priceRange'][data['SalePrice'] < minPrice + divs] = 0.0 #Economico\n",
    "data['priceRange'][data['SalePrice'] >= minPrice + divs] = 1.0 #Precio medio\n",
    "data['priceRange'][data['SalePrice'] >= minPrice + divs * 2] = 2.0 #Caro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['priceRange']\n",
    "X = data.drop(['priceRange'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento de variables cuantitativas y cualitativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocesador = ColumnTransformer([\n",
    "    ('one_hot_encoder',categorical_preprocessor,quali),\n",
    "    ('numerico', numeric_preprocessor,quant)\n",
    "],remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copieddata = data.copy()\n",
    "copieddata = copieddata.fillna(0)\n",
    "\n",
    "target = copieddata.pop('priceRange')\n",
    "data = copieddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "  if (data[item].dtype == 'object'):\n",
    "    data = data.astype({item: str})\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento + modelo en un paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"linear\"))\n",
    "\n",
    "set_config(display='diagram')\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "yTrain = y_train.apply(pd.to_numeric, errors='coerce')\n",
    "Xtrain.fillna(0, inplace=True)\n",
    "yTrain.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(Xtrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.score(Xtrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pred = modelo.predict(X_test)\n",
    "print(target_pred)\n",
    "print (\"Accuracy:\",metrics.accuracy_score(y_test, target_pred))\n",
    "print (\"Precision:\", metrics.precision_score(y_test,target_pred,average='weighted') )\n",
    "print (\"Recall: \", metrics.recall_score(y_test,target_pred,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix\n",
    "confusion_matrix(target_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste de validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "cv = 8\n",
    "cv_results = cross_validate(modelo,data, Y, cv=cv)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "print(\"accuracy: \"+str(cv_results['test_score'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pred = cross_val_predict(modelo, data, Y, cv = cv)\n",
    "confusion_matrix(Y, target_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la realización de la validación cruzada, se puede mejorar el sobreajuste que se debe al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"poly\"))\n",
    "_=modelo.fit(X_train,y_train)\n",
    "param_grid = {\n",
    "    'svc__C': (0.01, 0.1, 1, 5,16,32),\n",
    "    'svc__degree':(2,3,5,7)\n",
    "    }\n",
    "model_grid_search = GridSearchCV(modelo, param_grid=param_grid,\n",
    "                                 n_jobs=2, cv=5)\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"rbf\"))\n",
    "_=modelo.fit(X_train,y_train)\n",
    "param_grid = {\n",
    "    'svc__C': (0.01, 0.1, 1, 5,16,32),\n",
    "    'svc__gamma':(0.0000000002,0.00002,0.01,0.1,20,200)\n",
    "    }\n",
    "model_grid_search = GridSearchCV(modelo, param_grid=param_grid,\n",
    "                                 n_jobs=2, cv=5)\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the target for official scoring\n",
    "train = data[quant].dropna().copy()\n",
    "y = train.pop(\"SalePrice\")\n",
    "X = train #El resto de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X, y,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelosvr = SVR(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelosvr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-Support Vector Regression.\n",
    "y_pred = modelosvr.predict(X_test)\n",
    "score = modelosvr.score(X_train, y_train)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se transforman las colunas usando los preprocesadores\n",
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Se preparan los preprocesadores\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "preprocesador = ColumnTransformer([\n",
    "    ('one_hot_encoder', categorical_preprocessor, quali),\n",
    "    ('numerico', numeric_preprocessor, quant)\n",
    "],remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "  if (data[item].dtype == 'object'):\n",
    "    data = data.astype({item: str})\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"linear\"))\n",
    "modelo.fit(Xtrain,yTrain)\n",
    "print(modelo.score(X_test, y_test)) \n",
    "X_pred = modelo.predict(X_test)\n",
    "print (\"Accuracy:\",metrics.accuracy_score(y_test, X_pred))\n",
    "print (\"Precision:\", metrics.precision_score(y_test, X_pred,average='weighted') )\n",
    "print (\"Recall: \", metrics.recall_score(y_test, X_pred,average='weighted'))\n",
    "print(confusion_matrix(X_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"rbf\", C=32, degree=2e-05))\n",
    "modelo.fit(X_train,y_train)\n",
    "print(modelo.score(X_test, y_test))\n",
    "X_pred = modelo.predict(X_test)\n",
    "print (\"Accuracy:\",metrics.accuracy_score(y_test, X_pred))\n",
    "print (\"Precision:\", metrics.precision_score(y_test, X_pred,average='weighted') )\n",
    "print (\"Recall: \", metrics.recall_score(y_test, X_pred,average='weighted'))\n",
    "print(confusion_matrix(X_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"poly\", C=32, degree=4))\n",
    "modelo.fit(X_train,y_train)\n",
    "print(modelo.score(X_test, y_test))\n",
    "X_pred = modelo.predict(X_test)\n",
    "print (\"Accuracy:\",metrics.accuracy_score(y_test, X_pred))\n",
    "print (\"Precision:\", metrics.precision_score(y_test, X_pred,average='weighted') )\n",
    "print (\"Recall: \", metrics.recall_score(y_test, X_pred,average='weighted'))\n",
    "print(confusion_matrix(X_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar el recall generado en estos modelos está en un rango de 0.961-0.971 siendo el mayor el kernel lineal, con una distribución de 0.971, una exactitud de 0.972 y una presición de 0.971 y el modelo menos eficaz fue el modelo rbf. Así mismo, el modelo lienal es el que menos se demora en procesar, debido a que los datos no se proyectan en dimensiones más altas cuando se usa este núcleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6cb11118430048aaffde37140829a96b2f2778371a079ecf7cef1d9eee249f62"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
